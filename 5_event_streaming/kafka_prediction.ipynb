{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805f5566-4ed6-478f-82bd-219b1a61a992",
   "metadata": {},
   "source": [
    "# 5.0 Event Streaming\n",
    "\n",
    "###### Author: Yeap Jie Shen, Gan Yee Jing\n",
    "###### Last Edited: 02/09/2024\n",
    "\n",
    "## 5.3 Kafka Streaming (Predictions) \n",
    "### 5.2.1 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3673236-cbc6-48a9-a302-1caf759797a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:09 WARN Utils: Your hostname, Gan. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/02 17:38:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 17:38:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(r'/home/student/RDS2S3G4_CLO2_B')\n",
    "\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from data_stores.vectorArrayConverter import VectorArrayConverter\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName('Kafka Streaming').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c12a43-5d39-4964-aba7-293a51fe1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining variables and loading models\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(inputCols = ['1tf_idf_content', '1gram_word2vec_content'], outputCol = 'features')\n",
    "\n",
    "# Loading pretrained RandomForestClassificationModel\n",
    "rf_model = RandomForestClassificationModel.load(r'../model/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f571d12-f152-4c64-ba87-3d37e9f094e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribing to topic 'ProcessedCrimeNews'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:25 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:25 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:25 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|altantuyas family...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:26 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:30 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:31 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:31 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|ambank founders m...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:32 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n",
      "Processed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:35 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|excops death pena...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:36 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n",
      "Processed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:39 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:39 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:39 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|man pleads guilty...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:40 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n",
      "Processed 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:42 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:42 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:43 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|police arrest two...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:43 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to DrugOffences\n",
      "Processed 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:46 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:46 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:46 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|four charged with...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:47 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n",
      "Processed 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|november defence ...|       3.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:50 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:52 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:52 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:53 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|man walks free af...|       5.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:53 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:56 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:56 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:56 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|police uncover pa...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:57 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to MurderAndHomicide\n",
      "Processed 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:38:59 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:59 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:38:59 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|johor broker lose...|       2.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:39:00 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:39:02 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:39:03 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:39:03 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|businessman plead...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:39:04 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published to DrugOffences\n",
      "Processed 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:39:06 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:39:06 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/09/02 17:39:06 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            headline|prediction|\n",
      "+--------------------+----------+\n",
      "|asean secgen meet...|       3.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:39:07 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "# kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic DrugOffences\n",
    "# kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic MurderAndHomicide\n",
    "\n",
    "# kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic DrugOffences\n",
    "# kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic MurderAndHomicide\n",
    "\n",
    "# Define the Kafka topic to subscribe to\n",
    "topic_name = 'ProcessedCrimeNews'\n",
    "\n",
    "# Initialize the Kafka consumer\n",
    "consumer = KafkaConsumer(topic_name, bootstrap_servers = 'localhost:9092', auto_offset_reset = 'earliest', value_deserializer = lambda x: x.decode('utf-8'))\n",
    "\n",
    "# Initialize the Kafka Producer\n",
    "producer = KafkaProducer(value_serializer = lambda v: json.dumps(v).encode('utf-8'), bootstrap_servers = 'localhost:9092')\n",
    "\n",
    "# Consume messages from Kafka\n",
    "try:\n",
    "    print(f\"Subscribing to topic '{topic_name}'\")\n",
    "    count = 0\n",
    "    for message in consumer:\n",
    "        json_value = json.loads(message.value)\n",
    "        \n",
    "        df = spark.createDataFrame([Row(**json_value)])\n",
    "            \n",
    "        df = (\n",
    "            df\n",
    "            .withColumn('1tf_idf_content', VectorArrayConverter.array_to_vector('1tf_idf_content'))\n",
    "            .withColumn('1gram_word2vec_content', VectorArrayConverter.array_to_vector('1gram_word2vec_content'))\n",
    "        )\n",
    "\n",
    "        df = assembler.transform(df)\n",
    "        \n",
    "        # Use the loaded model to make predictions\n",
    "        df_predictions = rf_model.transform(df)\n",
    "\n",
    "        # Outputting relevant information\n",
    "        count += df_predictions.count()\n",
    "        print('Processed', count)\n",
    "        \n",
    "        df_predictions.select('headline', 'prediction').show()\n",
    "\n",
    "        df_predictions = (\n",
    "            df_predictions\n",
    "            .withColumn('1tf_idf_content', VectorArrayConverter.vector_to_array(df_predictions['1tf_idf_content']))\n",
    "            .withColumn('1gram_word2vec_content', VectorArrayConverter.vector_to_array(df_predictions['1gram_word2vec_content']))\n",
    "        )\n",
    "\n",
    "        prediction_list = (\n",
    "            df_predictions\n",
    "            .select('url', 'headline', 'datetime', 'author', 'publisher', '1tf_idf_content', '1gram_word2vec_content', 'prediction')\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        # 0 -> DrugOffences, 1 -> MurderAndHomicide\n",
    "        for row in prediction_list:\n",
    "            if row['prediction'] == 0.0:\n",
    "                producer.send(topic = 'DrugOffences', value = row)\n",
    "                print('Published to DrugOffences')\n",
    "            elif row['prediction'] == 1.0:\n",
    "                producer.send(topic = 'MurderAndHomicide', value = row)\n",
    "                print('Published to MurderAndHomicide')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda48e39-79fe-479c-9a9a-658c20675258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: 0.0 Headline: police arrest two traffickers seize rm mln in drugs     Publisher: Selangor Journal     Date Published: 2024-08-24T22:08:01+08:00 URL: https://selangorjournal.my/2024/08/police-arrest-two-traffickers-seize-rm2-35-mln-in-drugs/\n",
      "\n",
      "Predicted Category: 0.0 Headline: businessman pleads not guilty to cheating charge        Publisher: Selangor Journal     Date Published: 2024-08-29T09:28:22+08:00 URL: https://selangorjournal.my/2024/08/businessman-pleads-not-guilty-to-cheating-charge/\n",
      "\n",
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "drug_consumer = KafkaConsumer('DrugOffences', bootstrap_servers = 'localhost:9092', auto_offset_reset = 'earliest', value_deserializer = lambda x: x.decode('utf-8'))\n",
    "\n",
    "try:\n",
    "    for message in drug_consumer:\n",
    "        json_value = json.loads(message.value)\n",
    "\n",
    "        news = {\n",
    "            key: value \n",
    "            for (key, value) in \n",
    "            zip(['url', 'headline', 'datetime', 'author', 'publisher', '1tf_idf_content', '1gram_word2vec_content', 'prediction'], json_value)\n",
    "        }\n",
    "        \n",
    "        # print(json_value)\n",
    "        print(f\"Predicted Category: {news['prediction']} Headline: {news['headline']:55s} Publisher: {news['publisher']:20s} Date Published: {news['datetime']:20s} URL: {news['url']:50s}\")\n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "finally:\n",
    "    drug_consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a992f606-c77e-4a68-a13a-0ccf2f113be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: 1.0 Headline: altantuyas family files bankruptcy notice against razak baginda Publisher: Selangor Journal     Date Published: 2024-08-29T17:10:35+08:00 URL: https://selangorjournal.my/2024/08/altantuyas-family-files-bankruptcy-notice-against-razak-baginda/\n",
      "\n",
      "Predicted Category: 1.0 Headline: ambank founders murder federal court dismisses extow truck drivers death penalty review Publisher: Selangor Journal     Date Published: 2024-08-21T16:02:37+08:00 URL: https://selangorjournal.my/2024/08/ambank-founders-murder-federal-court-dismisses-ex-tow-truck-drivers-death-penalty-review/\n",
      "\n",
      "Predicted Category: 1.0 Headline: excops death penalty reinstated for stepdaughters murder Publisher: Selangor Journal     Date Published: 2024-08-29T14:05:50+08:00 URL: https://selangorjournal.my/2024/08/ex-cops-death-penalty-reinstated-for-stepdaughters-murder/\n",
      "\n",
      "Predicted Category: 1.0 Headline: man pleads guilty to kicking hitting wife with broom    Publisher: Selangor Journal     Date Published: 2024-08-23T16:08:42+08:00 URL: https://selangorjournal.my/2024/08/man-pleads-guilty-to-kicking-hitting-wife-with-broom/\n",
      "\n",
      "Predicted Category: 1.0 Headline: four charged with trafficking nearly kg of drugs        Publisher: Selangor Journal     Date Published: 2024-08-22T18:07:57+08:00 URL: https://selangorjournal.my/2024/08/four-charged-with-trafficking-nearly-60kg-of-drugs/\n",
      "\n",
      "Predicted Category: 1.0 Headline: police uncover pantheon ventures investment scam with nearly rm mln in losses Publisher: Selangor Journal     Date Published: 2024-08-17T10:18:05+08:00 URL: https://selangorjournal.my/2024/08/police-uncover-pantheon-ventures-investment-scam-with-nearly-rm15-mln-in-losses/\n",
      "\n",
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "murder_consumer = KafkaConsumer('MurderAndHomicide', bootstrap_servers = 'localhost:9092', auto_offset_reset = 'earliest', value_deserializer = lambda x: x.decode('utf-8'))\n",
    "\n",
    "try:\n",
    "    for message in murder_consumer:\n",
    "        json_value = json.loads(message.value)\n",
    "\n",
    "        news = {\n",
    "            key: value \n",
    "            for (key, value) in \n",
    "            zip(['url', 'headline', 'datetime', 'author', 'publisher', '1tf_idf_content', '1gram_word2vec_content', 'prediction'], json_value)\n",
    "        }\n",
    "        \n",
    "        # print(json_value)\n",
    "        print(f\"Predicted Category: {news['prediction']} Headline: {news['headline']:55s} Publisher: {news['publisher']:20s} Date Published: {news['datetime']:20s} URL: {news['url']:50s}\")\n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "finally:\n",
    "    murder_consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6b0d4f-b1cd-4bed-9a99-f075c078023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
