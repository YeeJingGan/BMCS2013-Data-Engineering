{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0c1892-47f0-4db8-9873-ba3a23e8b405",
   "metadata": {},
   "source": [
    "# 5.0 Event Streaming\n",
    "\n",
    "###### Author: Yeap Jie Shen, Gan Yee Jing\n",
    "###### Last Edited: 02/09/2024\n",
    "\n",
    "## 5.2 Spark Structured Streaming \n",
    "### 5.2.1 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778ce470-b38a-43c1-9139-b46a0e901943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:20 WARN Utils: Your hostname, Gan. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/02 17:36:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/student/de-prj/de-venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/student/.ivy2/cache\n",
      "The jars for the packages stored in: /home/student/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e264c65-2b63-47f9-a2f4-9d092e6b538a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 657ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9e264c65-2b63-47f9-a2f4-9d092e6b538a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/7ms)\n",
      "24/09/02 17:36:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 17:36:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDFModel, Word2VecModel, NGram, CountVectorizer, StringIndexer\n",
    "from pyspark.sql.functions import lower, regexp_replace, regexp_extract, udf, col, expr, from_json, size\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(r'/home/student/RDS2S3G4_CLO2_B')\n",
    "\n",
    "from data_stores.vectorArrayConverter import VectorArrayConverter\n",
    "\n",
    "# Creating spark session\n",
    "spark = SparkSession.builder.appName('structured streaming').config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5d624-5c0e-426d-82c6-3d371d43d5ea",
   "metadata": {},
   "source": [
    "### 5.2.2 Consuming Raw News From Producer and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6fa32c-4969-4c9c-bf4f-113326f72ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining variables and functions\n",
    "\n",
    "# Define schema for JSON data\n",
    "schema = StructType([\n",
    "    StructField('url', StringType(), True),\n",
    "    StructField('headline', StringType(), True),\n",
    "    StructField('datetime', StringType(), True),\n",
    "    StructField('content', StringType(), True),\n",
    "    StructField('publisher', StringType(), True),\n",
    "    StructField('author', StringType(), True)\n",
    "])\n",
    "\n",
    "content_tokenizer = Tokenizer(outputCol = 'content_tokens', inputCol = 'content') # tokeniser\n",
    "headline_tokenizer = Tokenizer(outputCol = 'headline_tokens', inputCol = 'headline') # tokeniser\n",
    "\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Define a UDF to filter words not in the English corpus\n",
    "@udf(returnType = ArrayType(StringType()))\n",
    "def filter_non_english_words(words):\n",
    "    return [word for word in words if word.lower() in english_words]\n",
    "\n",
    "content_stopword_remover = StopWordsRemover(inputCol= 'content_tokens', outputCol = 'cleaned_content_tokens') # Stopword remover\n",
    "headline_stopword_remover = StopWordsRemover(inputCol= 'headline_tokens', outputCol = 'cleaned_headline_tokens') # Stopword remover\n",
    "\n",
    "# Lemmatiser\n",
    "lemmatizer_broadcast = spark.sparkContext.broadcast(WordNetLemmatizer())\n",
    "\n",
    "@udf(returnType = ArrayType(StringType()))\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = lemmatizer_broadcast.value\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Function to remove tokens with character length less than 4\n",
    "@udf(returnType = ArrayType(StringType()))\n",
    "def remove_short_length_words(words):\n",
    "    return [word for word in words if len(word) > 3]\n",
    "\n",
    "# TF-IDF\n",
    "hashing_tf_content = HashingTF(inputCol='1gram_content', outputCol='1tf_content', numFeatures=20)\n",
    "idf_content_model = IDFModel.load(r'../model/1_idf_content')\n",
    "\n",
    "# Word2Vec\n",
    "word2vec = Word2VecModel.load(r'../model/1_gram_word2vec_content')\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "def count_records_stop(df_batch, batch_id):\n",
    "    global record_count\n",
    "    \n",
    "    batch_count = df_batch.count()\n",
    "    record_count += batch_count\n",
    "    \n",
    "\n",
    "    df_kafka = (\n",
    "        df_batch\n",
    "        .withColumn('1tf_idf_content', VectorArrayConverter.vector_to_array(df_batch['1tf_idf_content']))\n",
    "        .withColumn('1gram_word2vec_content', VectorArrayConverter.vector_to_array(df_batch['1gram_word2vec_content']))\n",
    "    )\n",
    "    \n",
    "    # Sink the batch to Kafka\n",
    "    df_kafka = df_kafka.selectExpr(\"to_json(struct(*)) AS value\")  # Convert the DataFrame to JSON format\n",
    "\n",
    "    df_kafka.write \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "        .option('topic', 'ProcessedCrimeNews') \\\n",
    "        .save()\n",
    "\n",
    "    print(f'Batch ID: {batch_id} consumed {batch_count} records (Total {record_count} records)')\n",
    "    df_batch.show()\n",
    "    print(f'Batch ID: {batch_id} sinked {batch_count} records (Total {record_count} records)')\n",
    "\n",
    "    if record_count > 10:\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52158f24-8e64-4291-bffc-716ac2e1d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2c496ff5-a0f4-48f4-80b8-ef225c693c86. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/02 17:36:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/09/02 17:36:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/09/02 17:36:37 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:38 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:38 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:40 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:41 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:41 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:41 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 5 : {ProcessedCrimeNews=LEADER_NOT_AVAILABLE}\n",
      "24/09/02 17:36:42 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 0 consumed 3 records (Total 3 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:42 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:42 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|                 url|            headline|            datetime|author|       publisher|       1gram_content|cleaned_headline_tokens|         1tf_content|     1tf_idf_content|1gram_word2vec_content|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|https://selangorj...|altantuyas family...|2024-08-29T17:10:...|      |Selangor Journal|[family, late, mo...|   [family, bankrupt...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.18192636084833...|\n",
      "|https://selangorj...|ambank founders m...|2024-08-21T16:02:...|      |Selangor Journal|[federal, court, ...|   [murder, federal,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.04813456745300...|\n",
      "|https://selangorj...|excops death pena...|2024-08-29T14:05:...|      |Selangor Journal|[federal, court, ...|   [death, penalty, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.10054898665597...|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "\n",
      "Batch ID: 0 sinked 3 records (Total 3 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:43 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:43 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:43 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:44 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:44 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:44 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 1 consumed 3 records (Total 6 records)\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|                 url|            headline|            datetime|author|       publisher|       1gram_content|cleaned_headline_tokens|         1tf_content|     1tf_idf_content|1gram_word2vec_content|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|https://selangorj...|man pleads guilty...|2024-08-23T16:08:...|      |Selangor Journal|[delivery, guilty...|   [guilty, kicking,...|(20,[0,1,2,3,5,6,...|(20,[0,1,2,3,5,6,...|  [-0.1826675417833...|\n",
      "|https://selangorj...|police arrest two...|2024-08-24T22:08:...|      |Selangor Journal|[police, drug, sy...|   [police, arrest, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.08117359207670...|\n",
      "|https://selangorj...|four charged with...|2024-08-22T18:07:...|      |Selangor Journal|[four, court, tod...|         [four, nearly]|(20,[0,2,3,4,5,6,...|(20,[0,2,3,4,5,6,...|  [3.27678049604098...|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "\n",
      "Batch ID: 1 sinked 3 records (Total 6 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:48 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:48 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:48 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 2 consumed 3 records (Total 9 records)\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|                 url|            headline|            datetime|author|       publisher|       1gram_content|cleaned_headline_tokens|         1tf_content|     1tf_idf_content|1gram_word2vec_content|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|https://selangorj...|november defence ...|2024-08-29T17:58:...|      |Selangor Journal|[court, three, st...|   [defence, unregis...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.02922681798537...|\n",
      "|https://selangorj...|man walks free af...|2024-08-22T18:14:...|      |Selangor Journal|[session, court, ...|   [free, offensive,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.02464955663798...|\n",
      "|https://selangorj...|police uncover pa...|2024-08-17T10:18:...|      |Selangor Journal|[police, uncovere...|   [police, uncover,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.22876355204081...|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "\n",
      "Batch ID: 2 sinked 3 records (Total 9 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 3 consumed 3 records (Total 12 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 17:36:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/09/02 17:36:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|                 url|            headline|            datetime|author|       publisher|       1gram_content|cleaned_headline_tokens|         1tf_content|     1tf_idf_content|1gram_word2vec_content|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "|https://selangorj...|johor broker lose...|2024-08-21T15:59:...|      |Selangor Journal|[broker, million,...|   [broker, bogus, f...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|  [0.27169035918139...|\n",
      "|https://selangorj...|businessman plead...|2024-08-29T09:28:...|      |Selangor Journal|[businessman, gui...|   [businessman, gui...|(20,[0,1,2,3,4,6,...|(20,[0,1,2,3,4,6,...|  [0.12104074949989...|\n",
      "|https://selangorj...|asean secgen meet...|2024-08-29T17:32:...|      |Selangor Journal|[home, minister, ...|   [discus, transnat...|(20,[0,2,3,4,5,6,...|(20,[0,2,3,4,5,6,...|  [0.14132028526033...|\n",
      "+--------------------+--------------------+--------------------+------+----------------+--------------------+-----------------------+--------------------+--------------------+----------------------+\n",
      "\n",
      "Batch ID: 3 sinked 3 records (Total 12 records)\n"
     ]
    }
   ],
   "source": [
    "# kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic ProcessedCrimeNews\n",
    "# kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic ProcessedCrimeNews\n",
    "\n",
    "# Read from Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "    .option('subscribe', 'CrimeNews') \\\n",
    "    .option('startingOffsets', 'earliest') \\\n",
    "    .option('maxOffsetsPerTrigger', 3) \\\n",
    "    .load() \n",
    "\n",
    "df = df_raw.selectExpr('CAST(value AS STRING)')\n",
    "\n",
    "# Parse JSON value to structured format and Extract fields from the structured format\n",
    "df = df.withColumn('value', from_json(df['value'], schema)).select('value.*')\n",
    "\n",
    "# Preprocess the data\n",
    "# Noise Removal\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('content', lower('content'))\n",
    "    .withColumn('headline', lower('headline'))\n",
    "    .withColumn('content', regexp_replace('content', r'[^\\d\\w\\s]+',''))\n",
    "    .withColumn('headline', regexp_replace('headline', r'[^\\d\\w\\s]+',''))\n",
    "    .withColumn('content', regexp_replace('content', r'\\d+', ''))\n",
    "    .withColumn('headline', regexp_replace('headline', r'\\d+', ''))\n",
    "    .withColumn('content', regexp_replace('content', '[\\U0001F600-\\U0001F64F]', ''))\n",
    "    .withColumn('headline', regexp_replace('headline', '[\\U0001F600-\\U0001F64F]', ''))\n",
    "    .withColumn('content', regexp_replace('content', '[^\\x00-\\x7F]+', ''))\n",
    "    .withColumn('headline', regexp_replace('headline', '[^\\x00-\\x7F]+', ''))\n",
    "    .withColumn('content', regexp_replace('content', r'\\s+',' '))\n",
    "    .withColumn('headline', regexp_replace('headline', r'\\s+',' '))\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "df = content_tokenizer.transform(df)\n",
    "df = headline_tokenizer.transform(df)\n",
    "\n",
    "# Remove non-english words\n",
    "df = df.withColumn('content_tokens', filter_non_english_words('content_tokens')).withColumn('headline_tokens', filter_non_english_words('headline_tokens'))\n",
    "\n",
    "# Remove stopword\n",
    "df = (\n",
    "    headline_stopword_remover\n",
    "    .transform(content_stopword_remover.transform(df))\n",
    "    .select('url', 'headline', 'datetime', 'author', 'publisher', 'cleaned_content_tokens', 'cleaned_headline_tokens')\n",
    ")\n",
    "\n",
    "# Lemmatisation\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('cleaned_content_tokens', lemmatize_words('cleaned_content_tokens'))\n",
    "    .withColumn('cleaned_headline_tokens', lemmatize_words('cleaned_headline_tokens'))\n",
    ")\n",
    "\n",
    "# Removing tokens with character length less than 4\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('cleaned_content_tokens', remove_short_length_words('cleaned_content_tokens'))\n",
    "    .withColumn('cleaned_headline_tokens', remove_short_length_words('cleaned_headline_tokens'))\n",
    ")\n",
    "\n",
    "df = df.withColumnRenamed('cleaned_content_tokens', '1gram_content')\n",
    "\n",
    "# TF-IDF\n",
    "df = hashing_tf_content.transform(df)\n",
    "df = idf_content_model.transform(df)\n",
    "\n",
    "# Word2Vec\n",
    "df = word2vec.transform(df)\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(count_records_stop) \\\n",
    "    .start()\n",
    "\n",
    "# Await termination of the query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bdd707-0012-4759-ae39-850740f7ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
